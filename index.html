<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Gestos con MediaPipe (corregido)</title>
  <style>
    body { margin: 0; background: #fafafa; display: flex; flex-direction: column; align-items: center; }
    #video {
      position: absolute;
      top: 50px;
      width: 640px;
      height: 480px;
      z-index: 1;
    }
    #canvas {
      position: absolute;
      top: 50px;
      width: 640px;
      height: 480px;
      z-index: 2;
      pointer-events: none;
    }
    #log {
      margin-top: 550px;
      width: 640px;
      font-size: 1.2em;
      color: #333;
    }
  </style>
</head>
<body>
  <h2>Detección de Gestos (MediaPipe)</h2>
  <!-- 1) Video con playsinline y dimensión explícita -->
  <video id="video" autoplay playsinline muted></video>
  <canvas id="canvas"></canvas>
  <div id="log">Esperando gesto...</div>

  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest"></script>
  <script>
    const { FilesetResolver, GestureRecognizer, GestureRecognizerModelConfig, VisionRunningMode } = window;

    async function init() {
      const visionFiles = await FilesetResolver.forVisionTasks(
        'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm'
      );

      const gestureRecognizer = await GestureRecognizer.createFromModelAndOptions(
        visionFiles,
        new GestureRecognizerModelConfig({
          modelAssetPath: 'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/gesture_recognizer.task',
          runningMode: VisionRunningMode.LIVE_STREAM,
          numHands: 2,
          minHandPresenceConfidence: 0.5,
          minTrackingConfidence: 0.5
        })
      );

      const video = document.getElementById('video');
      const canvas = document.getElementById('canvas');
      const ctx = canvas.getContext('2d');
      const log = document.getElementById('log');

      // 2) Solicitar cámara con dimensiones preferidas
      navigator.mediaDevices.getUserMedia({ video: { width: 640, height: 480 } })
        .then(stream => {
          video.srcObject = stream;
          video.addEventListener('loadedmetadata', () => {
            video.play();
            requestAnimationFrame(processFrame);
          });
        })
        .catch(err => {
          console.error('Error cámara:', err.name, err.message);
          log.textContent = 'No se pudo acceder a la cámara';
        });

      // 3) Procesar cada frame una vez que el vídeo esté listo
      async function processFrame() {
        const results = await gestureRecognizer.recognizeForVideo(video, performance.now());
        ctx.clearRect(0, 0, canvas.width, canvas.height);

        if (results.handedGestures.length) {
          results.multiHandLandmarks.forEach((landmarks, i) => {
            window.drawConnectors(ctx, landmarks, window.HAND_CONNECTIONS, { color:'#00FF00', lineWidth:2 });
            window.drawLandmarks(ctx, landmarks, { color:'#FF0000', lineWidth:1 });
            const best = results.handedGestures[i][0];
            log.textContent = `Mano ${i+1}: ${best.categoryName} (${(best.score*100).toFixed(1)}%)`;
          });
        } else {
          log.textContent = 'Sin gesto claro';
        }
        requestAnimationFrame(processFrame);
      }
    }

    init();
  </script>
</body>
</html>
