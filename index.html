<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <title>Detección de Gestos</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/handpose@0.0.7/dist/handpose.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface@0.0.7/dist/blazeface.min.js"></script>
    <style>
        body {
            margin: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            background-color: #f0f0f0;
        }
        .container {
            text-align: center;
        }
        video, canvas {
            max-width: 640px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        #detections {
            margin-top: 20px;
            padding: 15px;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
    </style>
</head>
<body>
    <div class="container">
        <video id="video" autoplay></video>
        <canvas id="canvas" style="position:absolute;top:0;left:0;"></canvas>
        <div id="detections"></div>
    </div>

    <script>
        async function init() {
            const video = document.getElementById('video');
            const canvas = document.getElementById('canvas');
            const ctx = canvas.getContext('2d');

            // Configuración de la cámara
            const stream = await navigator.mediaDevices.getUserMedia({ 
                video: { width: 640, height: 480 } 
            });
            video.srcObject = stream;

            // Inicialización de modelos
            const handModel = await handpose.load();
            const faceModel = await blazeface.load();

            async function detectAndDraw() {
                ctx.clearRect(0, 0, canvas.width, canvas.height);

                // Detección facial
                const predictionsFace = await faceModel.estimateFaces(video);
                
                // Dibujar rectángulos faciales
                predictionsFace.forEach(prediction => {
                    const start = prediction.topLeft;
                    const end = prediction.bottomRight;
                    
                    ctx.beginPath();
                    ctx.rect(start[0], start[1], end[0] - start[0], end[1] - start[1]);
                    ctx.strokeStyle = 'green';
                    ctx.lineWidth = 2;
                    ctx.stroke();

                    // Mostrar características faciales
                    const landmarks = prediction.landmarks;
                    drawLandmarks(ctx, landmarks);
                });

                // Detección de manos
                const predictionsHand = await handModel.estimateHands(video);
                
                predictionsHand.forEach(prediction => {
                    const keypoints = prediction.landmarks;
                    drawKeypoints(keypoints);
                });

                requestAnimationFrame(detectAndDraw);
            }

            function drawLandmarks(ctx, landmarks) {
                // Dibujar puntos clave del rostro
                ['mouth', 'leftEye', 'rightEye'].forEach(part => {
                    ctx.fillStyle = 'blue';
                    ctx.beginPath();
                    landmarks[part].forEach(point => {
                        ctx.arc(point[0], point[1], 2, 0, Math.PI * 2);
                        ctx.fill();
                    });
                });
            }

            function drawKeypoints(keypoints) {
                ctx.fillStyle = 'red';
                ctx.beginPath();
                keypoints.forEach(keypoint => {
                    ctx.arc(keypoint.x, keypoint.y, 2, 0, Math.PI * 2);
                    ctx.fill();
                });
            }

            // Iniciar detección
            video.addEventListener('loadeddata', () => {
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
                detectAndDraw();
            });
        }

        init().catch(console.error);
    </script>
</body>
</html>
